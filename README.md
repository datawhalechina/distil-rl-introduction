# üìö DistilRL: A Condensed Introduction

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](LICENSE)
[![Website](https://img.shields.io/badge/Website-Visit%20Site-blue?logo=github)](https://datawhalechina.github.io/distil-rl-introduction/)
[![About the Author](https://img.shields.io/badge/About%20the%20Author-GitHub-blue?logo=github)](https://github.com/Dong237)
[![GitHub Stars](https://img.shields.io/github/stars/Dong237/DistilRLIntroduction?style=social)](https://github.com/datawhalechina/distil-rl-introduction)

---

<span>[ English | <a href="README_zh.md">‰∏≠Êñá</a> ]</span>

## üìö What This Is

<img src="docs/\_static/img/logo.png" align="right" width="40%"/>

As someone who's been learning reinforcement learning (RL), I've been searching for resources that strike the right balance. 

*Reinforcement Learning: An Introduction* is the bible of RL, but reading it cover-to-cover takes a ton of effort.  

That's why I created this tutorial: a streamlined "knowledge vault" to help you absorb the core ideas faster and easier!

---

## üéØ Why This Project and how to use (for now)

> **üí° Key Idea:** This tutorial focuses on handpicked chapters from the RL Intro book and blends them with content from a [RL Coursera specialization](https://www.coursera.org/specializations/reinforcement-learning) for a smoother learning experience.


> **üõ†Ô∏è How to Use:** Read the whole tutorial on [this site](https://github.com/datawhalechina/distil-rl-introduction). You will find more details about how this project came to exist and how to best use it in Chapter 0. The Prelude!
---

## üìã Catalog

### üåü Introduction
- [Chapter 0: Prelude](docs/Contents/0_prelude.md)
- [Chapter 1: Introduction to RL](docs/Contents/1_intro.md)

### üßÆ Tabular Solution Methods
#### Fundamentals of Reinforcement Learning
- [Chapter 2: Multi-armed Bandits](docs/Contents/2_multi_armed_bandits.md)
- [Chapter 3: Markov Decision Process](docs/Contents/3_markov_decision_process.md)
- [Chapter 4: Dynamic Programming](docs/Contents/4_dynamic_programming.md)

#### Sample-based Learning Methods
- [Chapter 5: Monte Carlo Methods](docs/Contents/5_monte_carlo_methods.md)
- [Chapter 6: Temporal Difference Learning](docs/Contents/6_temporal_difference_learning.md)
- [Chapter 7: Planning, Learning, Acting](docs/Contents/7_planning_learning_acting.md)

### ü§ñ Approximate Solution Methods
#### Value Function Approximation
- [Chapter 8: On-policy Prediction with Approximation](docs/Contents/8_on_policy_prediction_with_approximation.md)
- [Chapter 9: On-policy Control with Approximation](docs/Contents/9_on_policy_control_with_approximation.md)

#### Policy Approximation
- [Chapter 10: Policy Gradient Methods](docs/Contents/10_policy_gradient_methods.md)
- [Chapter 11: Modern Policy Gradient Methods](docs/Contents/11_modern_policy_gradient_methods.md)

---

## ü§ù Contributing

I welcome contributions to improve this tutorial! Here's how you can help:

- **Report Issues**: Found a typo or unclear explanation? Open an issue!
- **Suggest Improvements**: Have ideas for better explanations or new content? Submit a pull request!
- **Spread the Word**: Share this tutorial with others who might find it useful!
- **Translation needed**: I am also trying to create a Chinese version of this book somewhen in the future, help me out if you are interested.

---

## üìú License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.